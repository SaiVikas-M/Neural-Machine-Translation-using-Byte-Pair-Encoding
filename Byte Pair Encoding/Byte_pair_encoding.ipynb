{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Byte_pair_encoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzVRHBdfKWNA",
        "outputId": "fd99a04a-4c6f-4f51-f811-787ee7d2e0db"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "        for line in fhand:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    if token[-4:] == '</w>':\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        return len(token)\n",
        "\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n",
        "\n",
        "# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
        "\n",
        "vocab = get_vocab('small_vocab_en.txt')\n",
        "\n",
        "print('==========')\n",
        "print('Tokens Before BPE')\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "print('==========')\n",
        "\n",
        "num_merges = 500\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    #print('Iter: {}'.format(i))\n",
        "    #print('Best pair: {}'.format(best))\n",
        "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "    #print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
        "    #print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
        "    #print('==========')\n",
        "\n",
        "# Let's check how tokenization will be for a known word\n",
        "word_given_known = 'strawberries</w>'\n",
        "word_given_unknown = 'apples</w>'\n",
        "\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "#print(sorted_tokens)\n",
        "\n",
        "word_given = word_given_known \n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_given))\n",
        "if word_given in vocab_tokenization:\n",
        "    print('Tokenization of the known word:')\n",
        "    print(vocab_tokenization[word_given])\n",
        "    print('Tokenization treating the known word as unknown:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "else:\n",
        "    print('Tokenizating of the unknown word:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "\n",
        "word_given = word_given_unknown \n",
        "\n",
        "print('Tokenizing word: {}...'.format(word_given))\n",
        "if word_given in vocab_tokenization:\n",
        "    print('Tokenization of the known word:')\n",
        "    print(vocab_tokenization[word_given])\n",
        "    print('Tokenization treating the known word as unknown:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "else:\n",
        "    print('Tokenizating of the unknown word:')\n",
        "    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========\n",
            "Tokens Before BPE\n",
            "All tokens: dict_keys(['n', 'e', 'w', '</w>', 'j', 'r', 's', 'y', 'i', 'o', 'm', 't', 'q', 'u', 'd', 'g', 'a', ',', 'p', 'l', '.', 'h', 'c', 'f', 'z', 'v', 'b', 'k', 'x', '?', \"'\"])\n",
            "Number of tokens: 31\n",
            "==========\n",
            "Tokenizing word: strawberries</w>...\n",
            "Tokenization of the known word:\n",
            "['strawberries</w>']\n",
            "Tokenization treating the known word as unknown:\n",
            "['strawberries</w>']\n",
            "Tokenizing word: apples</w>...\n",
            "Tokenization of the known word:\n",
            "['apples</w>']\n",
            "Tokenization treating the known word as unknown:\n",
            "['apples</w>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag-XeuepMZIs",
        "outputId": "ba3b45c1-cb9a-4e3c-dd6e-e9709169606c"
      },
      "source": [
        "dict = {}\n",
        "k = 0\n",
        "for i in sorted_tokens:\n",
        "  dict[i] = k\n",
        "  k += 1\n",
        "def tokenize(x,dict,sorted_tokens):\n",
        "  a = []\n",
        "  for i in x:\n",
        "    b = []\n",
        "    for j in i.split():\n",
        "      j = j+'</w>'\n",
        "      if j in vocab_tokenization:\n",
        "        b.append(dict[j])\n",
        "      else:\n",
        "        b.append(tokenize_word(string=j, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n",
        "    a.append(b)\n",
        "  return(a,dict)\n",
        "text_sentences = [\n",
        "    'new jersey is sometimes quiet during autumn and it is snowy in april',\n",
        "    'the united states is usually chilly during july and it is usually freezing in november']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences,dict,sorted_tokens)\n",
        "print(text_tokenizer)\n",
        "print(text_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'strawberries.</w>': 0, 'strawberries</w>': 1, 'grapefruit.</w>': 2, 'translating</w>': 3, 'strawberry.</w>': 4, 'california</w>': 5, 'grapefruit</w>': 6, 'strawberry</w>': 7, 'automobile</w>': 8, 'portuguese</w>': 9, 'sometimes</w>': 10, 'september</w>': 11, 'beautiful</w>': 12, 'wonderful</w>': 13, 'favorite.</w>': 14, 'translate</w>': 15, 'difficult</w>': 16, 'elephants</w>': 17, 'favorite</w>': 18, 'november</w>': 19, 'december</w>': 20, 'february</w>': 21, 'freezing</w>': 22, 'pleasant</w>': 23, 'relaxing</w>': 24, 'dislikes</w>': 25, 'disliked</w>': 26, 'peaches.</w>': 27, 'bananas.</w>': 28, 'oranges.</w>': 29, 'mangoes.</w>': 30, 'elephant</w>': 31, 'football</w>': 32, 'usually</w>': 33, 'january</w>': 34, 'october</w>': 35, 'mangoes</w>': 36, 'oranges</w>': 37, 'bananas</w>': 38, 'peaches</w>': 39, 'dislike</w>': 40, 'driving</w>': 41, 'animals</w>': 42, 'between</w>': 43, 'apples.</w>': 44, 'grapes.</w>': 45, 'spanish</w>': 46, 'chinese</w>': 47, 'english</w>': 48, 'lemons.</w>': 49, 'orange.</w>': 50, 'banana.</w>': 51, 'rabbits</w>': 52, 'monkeys</w>': 53, 'grocery</w>': 54, 'weather</w>': 55, 'during</w>': 56, 'united</w>': 57, 'states</w>': 58, 'jersey</w>': 59, 'france</w>': 60, 'spring</w>': 61, 'winter</w>': 62, 'autumn</w>': 63, 'summer</w>': 64, 'august</w>': 65, 'chilly</w>': 66, 'lemons</w>': 67, 'grapes</w>': 68, 'apples</w>': 69, 'banana</w>': 70, 'orange</w>': 71, 'animal</w>': 72, 'little</w>': 73, 'yellow</w>': 74, 'feared</w>': 75, 'drives</w>': 76, 'liked.</w>': 77, 'loved.</w>': 78, 'pears.</w>': 79, 'wanted</w>': 80, 'thinks</w>': 81, 'french</w>': 82, 'limes.</w>': 83, 'mango.</w>': 84, 'peach.</w>': 85, 'lemon.</w>': 86, 'apple.</w>': 87, 'monkey</w>': 88, 'rabbit</w>': 89, 'grape.</w>': 90, 'fruit.</w>': 91, 'horses</w>': 92, 'sharks</w>': 93, 'snakes</w>': 94, \"didn't</w>\": 95, 'eiffel</w>': 96, 'school</w>': 97, \"aren't</w>\": 98, 'never</w>': 99, 'least</w>': 100, 'fruit</w>': 101, 'loved</w>': 102, 'liked</w>': 103, 'paris</w>': 104, 'india</w>': 105, 'china</w>': 106, 'march</w>': 107, 'april</w>': 108, 'their</w>': 109, 'snowy</w>': 110, 'rainy</w>': 111, 'quiet</w>': 112, 'likes</w>': 113, 'limes</w>': 114, 'pears</w>': 115, 'grape</w>': 116, 'apple</w>': 117, 'lemon</w>': 118, 'mango</w>': 119, 'peach</w>': 120, 'truck</w>': 121, 'visit</w>': 122, 'rusty</w>': 123, 'white</w>': 124, 'black</w>': 125, 'green</w>': 126, 'shiny</w>': 127, 'going</w>': 128, 'drove</w>': 129, 'plans</w>': 130, 'might</w>': 131, 'wants</w>': 132, 'think</w>': 133, 'pear.</w>': 134, 'shark</w>': 135, 'mouse</w>': 136, 'horse</w>': 137, 'snake</w>': 138, 'lime.</w>': 139, 'bears</w>': 140, 'birds</w>': 141, 'lions</w>': 142, 'tower</w>': 143, 'store</w>': 144, 'field</w>': 145, 'would</w>': 146, \"isn't</w>\": 147, 'where</w>': 148, 'most</w>': 149, 'your</w>': 150, 'fall</w>': 151, 'june</w>': 152, 'nice</w>': 153, 'july</w>': 154, 'warm</w>': 155, 'cold</w>': 156, 'busy</w>': 157, 'mild</w>': 158, 'lime</w>': 159, 'pear</w>': 160, 'like</w>': 161, 'they</w>': 162, 'that</w>': 163, 'next</w>': 164, 'blue</w>': 165, 'last</w>': 166, 'this</w>': 167, 'plan</w>': 168, 'were</w>': 169, 'went</w>': 170, 'easy</w>': 171, \"it's</w>\": 172, 'bird</w>': 173, 'lion</w>': 174, 'bear</w>': 175, 'when</w>': 176, 'want</w>': 177, 'dogs</w>': 178, 'cats</w>': 179, 'mice</w>': 180, 'lake</w>': 181, 'been</w>': 182, 'does</w>': 183, 'have</w>': 184, 'the</w>': 185, 'but</w>': 186, 'and</w>': 187, 'new</w>': 188, 'she</w>': 189, 'his</w>': 190, 'her</w>': 191, 'may</w>': 192, 'our</w>': 193, 'dry</w>': 194, 'wet</w>': 195, 'hot</w>': 196, 'you</w>': 197, 'car</w>': 198, 'was</w>': 199, 'big</w>': 200, 'old</w>': 201, 'red</w>': 202, 'are</w>': 203, 'saw</w>': 204, 'fun</w>': 205, 'why</w>': 206, 'did</w>': 207, 'cat</w>': 208, 'dog</w>': 209, 'how</w>': 210, 'has</w>': 211, 'is</w>': 212, 'in</w>': 213, 'it</w>': 214, 'he</w>': 215, 'my</w>': 216, 'to</w>': 217, 'we</w>': 218, 'go</w>': 219, 'do</w>': 220, 'am</w>': 221, ',</w>': 222, '.</w>': 223, 'i</w>': 224, 'a</w>': 225, '?</w>': 226}\n",
            "[[188, 59, 212, 10, 112, 56, 63, 187, 214, 212, 110, 213, 108], [185, 57, 58, 212, 33, 66, 56, 154, 187, 214, 212, 33, 22, 213, 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}