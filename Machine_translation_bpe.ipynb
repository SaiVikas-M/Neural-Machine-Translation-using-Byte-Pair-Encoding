{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Machine_translation_bpe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D6_J-Uxlvb-"
      },
      "source": [
        "# Machine Translation using byte pair encoding\n",
        "\n",
        "### Introduction\n",
        "In this notebook, we will see how to create a language translation model which is an application of neural machine translation. We will be using the packages from keras at first and we will also test the same model and dataset with Byte-pair encoding for better results. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzML3GtyJMAg"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "Out first objective for this is to load the libraries and get the required data. For this project we have a french text file and an english text document each having 137860 sentences. In language translation projects this can be considered as a moderately sized data as we find models with very large data. We are currently using GPU provided by Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeq3uQp1l1Ef"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "import helper\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import L1L2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb8Uk14Nl8OB"
      },
      "source": [
        "### Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSznnGr7l-X7",
        "outputId": "e2db3056-a152-449e-f972-b8f87f9f31a3"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4001446454133006743\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14509932544\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 608189705821413393\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B7T6c8LmAhZ"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "All the data can be found in the same github repository. We first load the data from our text files into english_sentences and french_sentences sentences respectively by reading each line of the text file by cleaning the line. We have found a preprocessed clean dataset we just need to do small changes to it like removing the unnecessary trailing spaces, converting the text to lower case and removing punctuation to create better bag of vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH7BBWgfmDXD",
        "outputId": "9004f698-ad19-41e8-c003-5df1b60a3599"
      },
      "source": [
        "# Load English data\n",
        "eng_file = 'English_data.txt'\n",
        "fre_file = 'French_data.txt'\n",
        "english_sentences = []\n",
        "max_len = 0\n",
        "with open(eng_file) as f:\n",
        "    for line in f:\n",
        "      if len(line) > max_len:\n",
        "        max_len = len(line)\n",
        "      line = line.rstrip()\n",
        "      line = line.lower()\n",
        "      line = re.sub(r'[^\\w\\s]', '', line)\n",
        "      english_sentences.append(line)\n",
        "# Load French data\n",
        "french_sentences = []\n",
        "with open(fre_file) as f:\n",
        "    for line in f:\n",
        "      if len(line) > max_len:\n",
        "        max_len = len(line)\n",
        "      line = line.rstrip()\n",
        "      line = line.lower()\n",
        "      line = re.sub(r'[^\\w\\s]', '', line)\n",
        "      french_sentences.append(line)\n",
        "print('Dataset Loaded')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA1_jMzqMar3"
      },
      "source": [
        "### Sample english and french data in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAPFT0jcmPhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff863f46-48e1-4f76-e48f-8b614c1e0fe7"
      },
      "source": [
        "for sample_i in range(2):\n",
        "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn  and it is snowy in april \n",
            "small_vocab_fr Line 1:  new jersey est parfois calme pendant l automne  et il est neigeux en avril \n",
            "small_vocab_en Line 2:  the united states is usually chilly during july  and it is usually freezing in november \n",
            "small_vocab_fr Line 2:  les étatsunis est généralement froid en juillet  et il gèle habituellement en novembre \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW5Ag2vZmFZb"
      },
      "source": [
        "## Tokenize From Keras\n",
        "\n",
        "Tokens are the building blocks of Natural Language. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. Tokenization is a way of separating a piece of text into smaller units called tokens. Tokenization can be broadly classified into 3 types – word, character, and subword tokenization. \n",
        "\n",
        "Keras offers a library for word tokenization which can be used to create a bag of words which has all the words present in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB7wKDeymdL9"
      },
      "source": [
        "def tokenize(x):\n",
        "\n",
        "    # Create the tokeninzer\n",
        "    t = Tokenizer()\n",
        "    # Create dictionary mapping words (str) to their rank/index (int)\n",
        "    t.fit_on_texts(x)\n",
        "    # Use the tokenizer to tokenize the text\n",
        "    text_sequences = t.texts_to_sequences(x)\n",
        "    return text_sequences, t"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymRvBXYmhD_"
      },
      "source": [
        "## Padding\n",
        "\n",
        "All the neural networks require to have inputs that have the same shape and size. But, we can't expect all the sentences to be of the same length. So, we use padding. Padding is the process of adding the zeros to the sequence to make all the samples in the same size.  \n",
        "\n",
        "We import pad_sequences from keras.preprocessing.sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ntkRXERmn8x"
      },
      "source": [
        "def pad(x, length=None):\n",
        "    \n",
        "    # If length equals None, set it to be the length of the longest sequence in x\n",
        "    if length == None:\n",
        "        length = len(max(x, key=len))\n",
        "        \n",
        "    # Use Keras's pad_sequences to pad the sequences with 0's\n",
        "    padded_sequences = pad_sequences(sequences=x, maxlen=length, padding='post', value=0)\n",
        "    \n",
        "    return padded_sequences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfkzrvVcmq_C"
      },
      "source": [
        "## Data Pre-processing of input data\n",
        "\n",
        "RNN models wont accept text as input they olnly take sequences of integers. So, we'll convert the text into sequences of integers using the following preprocess methods:\n",
        "\n",
        "1. Tokenize the words into ids\n",
        "2. Add padding to make all the sequences the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI3V2bB1mYgC"
      },
      "source": [
        "def preprocess(x, y):\n",
        "\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "    \n",
        "english_sentences_train, english_sentences_test, french_sentences_train, french_sentences_test =\\\n",
        "    train_test_split(english_sentences, french_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "preproc_english_sentences_train, preproc_french_sentences_train, english_tokenizer_train, french_tokenizer_train =\\\n",
        "    preprocess(english_sentences_train, french_sentences_train)\n",
        "    \n",
        "max_english_sequence_length_train = preproc_english_sentences_train.shape[1]\n",
        "max_french_sequence_length_train = preproc_french_sentences_train.shape[1]\n",
        "english_vocab_size_train = len(english_tokenizer_train.word_index)\n",
        "french_vocab_size_train = len(french_tokenizer_train.word_index)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHbwzqSnSrpb"
      },
      "source": [
        "### Tokeniztion of unknown sentences\n",
        "\n",
        "The below code displays how the sentence which is not used for tokenization i is tokenized into a sequence of integers. If you observe, some of the lists are empty. It is because of a problem called Out of Vocabulary (OOV). Out-of-vocabulary (OOV) are terms that are not part of the tokenized words found in a natural language processing environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REbUZbqonU1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4447e2b7-97ae-4007-8a8e-604b9d2dd869"
      },
      "source": [
        "Unknown_sentence = 'he dislikes grapefruit limes and lemons'\n",
        "Tok_data = tokenize(Unknown_sentence)\n",
        "print(Tok_data)\n",
        "## All the empty values represent Unknown data"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([[10], [1], [], [5], [2], [3], [4], [2], [11], [1], [3], [], [12], [6], [7], [13], [1], [14], [6], [15], [2], [16], [], [4], [2], [8], [1], [3], [], [7], [9], [5], [], [4], [1], [8], [17], [9], [3]], <keras_preprocessing.text.Tokenizer object at 0x7fad4fcf7910>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuhE_dF6mv73"
      },
      "source": [
        "## Byte-Pair Encoding\n",
        "\n",
        "Byte-Pair Encoding aims to reduce the Out of Vocabulary words in a model with limited words in the dictionary by trying to combine word and character tokenization techniques. For example, if the model learned the relationship between \"old\", \"older\", and \"oldest\" then it could easily translate the word \"smarter\" or \"smartest\" if the dictionary has the word \"smart\" in it. A similar method can be used for a lot of words like words ending with \"ing\" (eat, eating), \"ed\" (learn, learned). We still can't tokenize for some words but it can tokenize more words than word-tokenization can which can be shown by using both the tokenizations on the same RNN model.\n",
        "\n",
        "The steps needed for BPE are\n",
        "\n",
        "1. Initialize vocabulary.\n",
        "2. Represent each word as a combination of the characters along with the special end of word token </w>.\n",
        "3. Iteratively count character pairs in all tokens of the vocabulary.\n",
        "4. Merge every occurrence of the most frequent pair, add the new character n-gram to the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrBTrj2fpshL"
      },
      "source": [
        "# Byte-Pair Encoding algorithm\n",
        "def tokenize_bpe(filename):\n",
        "  def get_vocab(filename):\n",
        "    # Creates a vocab from file\n",
        "    # Splits the word, removes punctuation, makes it lower case and adds </w> at the end\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            line = re.sub(r'[^\\w\\s]', '', line)\n",
        "            line = line.lower()\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "  def get_stats(vocab):\n",
        "      pairs = collections.defaultdict(int)\n",
        "      for word, freq in vocab.items():\n",
        "          symbols = word.split()\n",
        "          for i in range(len(symbols)-1):\n",
        "              pairs[symbols[i],symbols[i+1]] += freq\n",
        "      return pairs\n",
        "\n",
        "  def merge_vocab(pair, v_in):\n",
        "      v_out = {}\n",
        "      bigram = re.escape(' '.join(pair))\n",
        "      p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "      for word in v_in:\n",
        "          w_out = p.sub(''.join(pair), word)\n",
        "          v_out[w_out] = v_in[word]\n",
        "      return v_out\n",
        "\n",
        "  def get_tokens_from_vocab(vocab):\n",
        "      tokens_frequencies = collections.defaultdict(int)\n",
        "      vocab_tokenization = {}\n",
        "      for word, freq in vocab.items():\n",
        "          word_tokens = word.split()\n",
        "          for token in word_tokens:\n",
        "              tokens_frequencies[token] += freq\n",
        "          vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "      return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "  def measure_token_length(token):\n",
        "      if token[-4:] == '</w>':\n",
        "          return len(token[:-4]) + 1\n",
        "      else:\n",
        "          return len(token)\n",
        "\n",
        "  vocab = get_vocab(filename)\n",
        "  tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "\n",
        "  num_merges = 10000\n",
        "  for i in range(num_merges):\n",
        "      pairs = get_stats(vocab)\n",
        "      if not pairs:\n",
        "          break\n",
        "      best = max(pairs, key=pairs.get)\n",
        "      vocab = merge_vocab(best, vocab)\n",
        "      #print('Iter: {}'.format(i))\n",
        "      #print('Best pair: {}'.format(best))\n",
        "      tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
        "  return(tokens_frequencies, vocab_tokenization)\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "      \n",
        "    if string == '':\n",
        "        return []\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token]\n",
        "\n",
        "    string_tokens = []\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        #token_reg = re.escape(token.replace('.', '[.]'))\n",
        "\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token, string)]\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "\n",
        "        substring_start_position = 0\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    return string_tokens\n",
        "def get_idx(x,sorted_tokens):\n",
        "  a = []\n",
        "  for i in x:\n",
        "    b = []\n",
        "    for j in i.split():\n",
        "      j = j+'</w>'\n",
        "      if j in sorted_tokens:\n",
        "        b.append(sorted_tokens[j])\n",
        "      else:\n",
        "        temp = tokenize_word(string=j, sorted_tokens=list(sorted_tokens.keys()), unknown_token='</u>')\n",
        "        if temp:\n",
        "          b.append(temp)\n",
        "        else:\n",
        "          b.append(0)\n",
        "    a.append(b)\n",
        "  return(a,list(sorted_tokens.keys()))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6AY-ar9bga_"
      },
      "source": [
        "### Tokenizing input data\n",
        "\n",
        "Here we tokenize the input french and English sentences using Byte Pair Encoding. Here we are taking the number of merges as 10000, this is a hyper-parameter we set as per our data. As mentioned earlier, we append each word by '</w>'. This helps to recognize the end of words as the sentences are divided into letters before checking for pairs and '</w>' is used as the end of a word phrase. We also create a dictionary of words with indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CLQyM-1pzPG"
      },
      "source": [
        "tokens_frequencies_en, vocab_tokenization_en = tokenize_bpe(eng_file)\n",
        "tokens_frequencies_fr, vocab_tokenization_fr = tokenize_bpe(fre_file)\n",
        "k = 1\n",
        "for i in vocab_tokenization_en:\n",
        "  vocab_tokenization_en[i] = k\n",
        "  k += 1\n",
        "k = 1\n",
        "for i in vocab_tokenization_fr:\n",
        "  vocab_tokenization_fr[i] = k\n",
        "  k += 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cs5VllFalDf"
      },
      "source": [
        "### Tokeniztion of unknown sentences\n",
        "\n",
        "The below code displays how the sentence which is not used for tokenization is tokenized into a sequence of integers using Byte pair encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APfdUUAkqXQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bf6e42-226f-4a33-8612-98a8c594f41f"
      },
      "source": [
        "text_sentences = [\n",
        "    'new jersey is sometimes quiet during autumn and it is snowy in april',\n",
        "    'he liked grapefruit limes and lemons']\n",
        "text_tokenized, text_tokenizer = get_idx(text_sentences,vocab_tokenization_en)\n",
        "print(text_tokenizer)\n",
        "print(text_tokenized)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['new</w>', 'jersey</w>', 'is</w>', 'sometimes</w>', 'quiet</w>', 'during</w>', 'autumn</w>', 'and</w>', 'it</w>', 'snowy</w>', 'in</w>', 'april</w>', 'the</w>', 'united</w>', 'states</w>', 'usually</w>', 'chilly</w>', 'july</w>', 'freezing</w>', 'november</w>', 'california</w>', 'march</w>', 'hot</w>', 'june</w>', 'mild</w>', 'cold</w>', 'september</w>', 'your</w>', 'least</w>', 'liked</w>', 'fruit</w>', 'grape</w>', 'but</w>', 'my</w>', 'apple</w>', 'his</w>', 'favorite</w>', 'orange</w>', 'paris</w>', 'relaxing</w>', 'december</w>', 'busy</w>', 'spring</w>', 'never</w>', 'our</w>', 'lemon</w>', 'january</w>', 'warm</w>', 'lime</w>', 'her</w>', 'banana</w>', 'he</w>', 'saw</w>', 'a</w>', 'old</w>', 'yellow</w>', 'truck</w>', 'india</w>', 'rainy</w>', 'that</w>', 'cat</w>', 'was</w>', 'most</w>', 'loved</w>', 'animal</w>', 'dislikes</w>', 'grapefruit</w>', 'limes</w>', 'lemons</w>', 'february</w>', 'china</w>', 'pleasant</w>', 'october</w>', 'wonderful</w>', 'nice</w>', 'summer</w>', 'france</w>', 'may</w>', 'grapes</w>', 'mangoes</w>', 'their</w>', 'mango</w>', 'pear</w>', 'august</w>', 'beautiful</w>', 'apples</w>', 'peaches</w>', 'feared</w>', 'shark</w>', 'wet</w>', 'dry</w>', 'we</w>', 'like</w>', 'oranges</w>', 'they</w>', 'pears</w>', 'she</w>', 'little</w>', 'red</w>', 'winter</w>', 'disliked</w>', 'rusty</w>', 'car</w>', 'strawberries</w>', 'i</w>', 'strawberry</w>', 'bananas</w>', 'going</w>', 'to</w>', 'next</w>', 'plan</w>', 'visit</w>', 'elephants</w>', 'were</w>', 'animals</w>', 'are</w>', 'likes</w>', 'dislike</w>', 'fall</w>', 'driving</w>', 'peach</w>', 'drives</w>', 'blue</w>', 'you</w>', 'bird</w>', 'horses</w>', 'mouse</w>', 'went</w>', 'last</w>', 'horse</w>', 'automobile</w>', 'dogs</w>', 'white</w>', 'elephant</w>', 'black</w>', 'think</w>', 'difficult</w>', 'translate</w>', 'between</w>', 'spanish</w>', 'portuguese</w>', 'big</w>', 'green</w>', 'translating</w>', 'fun</w>', 'where</w>', 'dog</w>', 'why</w>', 'might</w>', 'go</w>', 'this</w>', 'drove</w>', 'shiny</w>', 'sharks</w>', 'monkey</w>', 'how</w>', 'weather</w>', 'lion</w>', 'plans</w>', 'bear</w>', 'rabbit</w>', 'its</w>', 'chinese</w>', 'when</w>', 'eiffel</w>', 'tower</w>', 'did</w>', 'grocery</w>', 'store</w>', 'wanted</w>', 'does</w>', 'football</w>', 'field</w>', 'wants</w>', 'didnt</w>', 'snake</w>', 'snakes</w>', 'do</w>', 'easy</w>', 'thinks</w>', 'english</w>', 'french</w>', 'would</w>', 'arent</w>', 'cats</w>', 'rabbits</w>', 'has</w>', 'been</w>', 'monkeys</w>', 'lake</w>', 'bears</w>', 'school</w>', 'birds</w>', 'want</w>', 'isnt</w>', 'lions</w>', 'am</w>', 'mice</w>', 'have</w>']\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 10, 11, 12], [52, 30, 67, 68, 8, 69]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTV_6memLRy"
      },
      "source": [
        "### Data preprocessing using BPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqQrge5ezzZL"
      },
      "source": [
        "def preprocess_BPE(english_sentences, french_sentences):\n",
        "  preprocess_x, x_tk = get_idx(english_sentences,vocab_tokenization_en)\n",
        "  preprocess_y, y_tk = get_idx(french_sentences,vocab_tokenization_fr)\n",
        "\n",
        "  preprocess_x = pad_sequences(preprocess_x,maxlen=max_len,padding='post')\n",
        "  preprocess_y = pad_sequences(preprocess_y,maxlen=max_len,padding='post')\n",
        "  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "  return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences_bpe, preproc_french_sentences_bpe, english_tokenizer_bpe, french_tokenizer_bpe=preprocess_BPE(english_sentences_train, french_sentences_train)\n",
        "english_vocab_size_bpe = len(english_tokenizer_bpe)\n",
        "french_vocab_size_bpe = len(french_tokenizer_bpe)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6iO1Uyk0FqQ"
      },
      "source": [
        "## Models\n",
        "This sectionis to demonstrate word tokenization and byte pair encoding for two models\n",
        "\n",
        "- Model 1 is a simple RNN (Word tokenization, BPE)\n",
        "- Model 2 is a RNN with Embedding (BPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcbJMhRiwm6K"
      },
      "source": [
        "### Ids Back to Text\n",
        "\n",
        "The neural network will be translating the input to a sequence of unreadable numbers, which isn't the final form we want. We want the translated french sentences.  The function 'logits_to_text' will convert the output integer sequences from the neural network to the French words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD2vcazuz_nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53d22c3-196c-4116-cd6f-a90a2230265c"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8UhYSi1ptvT"
      },
      "source": [
        "### RNN Models\n",
        "\n",
        "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French.\n",
        "\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2mxhe480Rz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a054a4-5aca-4555-9fd8-d6f628bd933b"
      },
      "source": [
        "def embed_model_split(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    # RNN model with embedding layer\n",
        "    input_layer = Input(shape=input_shape[1:])\n",
        "    embedding_layer = Embedding(512, english_vocab_size)(input_layer)\n",
        "    x = GRU(512, return_sequences=True)(embedding_layer)\n",
        "    x = TimeDistributed(Dense(french_vocab_size*4, activation='relu'))(x)\n",
        "    output_layer = Dense(french_vocab_size, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    # Compile the model\n",
        "    learning_rate = 0.01\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Pad the input \n",
        "tmp_x = pad(preproc_english_sentences_train, preproc_french_sentences_train.shape[1])\n",
        "\n",
        "# Train the neural network on the training data\n",
        "embedding_model_split_train = embed_model_split(tmp_x.shape,\n",
        "                                  max_french_sequence_length_train,\n",
        "                                  english_vocab_size_train + 1,\n",
        "                                  french_vocab_size_train + 1)\n",
        "\n",
        "hist = embedding_model_split_train.fit(tmp_x, preproc_french_sentences_train, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(embedding_model_split_train.predict(tmp_x[:1])[0], french_tokenizer_train))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "87/87 [==============================] - 32s 114ms/step - loss: 3.8145 - accuracy: 0.5129 - val_loss: 0.6742 - val_accuracy: 0.7976\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 9s 107ms/step - loss: 0.5613 - accuracy: 0.8251 - val_loss: 0.3966 - val_accuracy: 0.8666\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 9s 109ms/step - loss: 0.3684 - accuracy: 0.8759 - val_loss: 0.3244 - val_accuracy: 0.8900\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 10s 110ms/step - loss: 0.3005 - accuracy: 0.8973 - val_loss: 0.2776 - val_accuracy: 0.9044\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 10s 112ms/step - loss: 0.2606 - accuracy: 0.9097 - val_loss: 0.2645 - val_accuracy: 0.9090\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 10s 113ms/step - loss: 0.2446 - accuracy: 0.9145 - val_loss: 0.2568 - val_accuracy: 0.9110\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 10s 112ms/step - loss: 0.2371 - accuracy: 0.9167 - val_loss: 0.2489 - val_accuracy: 0.9150\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 10s 111ms/step - loss: 0.2271 - accuracy: 0.9196 - val_loss: 0.2471 - val_accuracy: 0.9152\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 10s 110ms/step - loss: 0.2219 - accuracy: 0.9212 - val_loss: 0.2409 - val_accuracy: 0.9171\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 10s 110ms/step - loss: 0.2159 - accuracy: 0.9229 - val_loss: 0.2341 - val_accuracy: 0.9193\n",
            "la france est généralement froid à lautomne mais il est calme en février <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MGNCnZGrfuq"
      },
      "source": [
        "### Ids Back to Text BPE\n",
        "\n",
        "Here we use a similar logits_to_text function to convert the output integer sequences from the neural network to the French words. Since, we used different tokenization methods we had to change the function a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atAGWdrFvqY_"
      },
      "source": [
        "def logits_to_text_bpe(logits, index_to_words):\n",
        "\n",
        "    index_to_words = {v: k for k, v in index_to_words.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    \n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKKPkbx44sBl"
      },
      "source": [
        "### RNN for BPE\n",
        "\n",
        "We use the same model for BPE as well. So, we can  check the accuracy and compare the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGp0icXOYUDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38830672-00f7-441c-d3a2-baa5c325d93a"
      },
      "source": [
        "preproc_english_sentences_bpe, preproc_french_sentences_bpe, english_tokenizer_bpe, french_tokenizer_bpe=preprocess_BPE(english_sentences_test, french_sentences_test)\n",
        "\n",
        "tmp_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "embedding_model_split_train = embed_model_split(tmp_x.shape,\n",
        "                                  max_french_sequence_length_train,\n",
        "                                  english_vocab_size_train + 1,\n",
        "                                  french_vocab_size_train + 1)\n",
        "\n",
        "hist_bpe = embedding_model_split_train.fit(tmp_x, preproc_french_sentences_bpe, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text_bpe(embedding_model_split_train.predict(tmp_x[:1])[0], vocab_tokenization_fr))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 15s 608ms/step - loss: 6.7716 - accuracy: 0.6179 - val_loss: nan - val_accuracy: 0.8936\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 13s 608ms/step - loss: 2.5988 - accuracy: 0.8911 - val_loss: nan - val_accuracy: 0.8972\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 13s 614ms/step - loss: 0.8403 - accuracy: 0.8984 - val_loss: nan - val_accuracy: 0.9015\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 13s 604ms/step - loss: 0.5749 - accuracy: 0.9041 - val_loss: nan - val_accuracy: 0.9055\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 13s 596ms/step - loss: 0.4518 - accuracy: 0.9070 - val_loss: nan - val_accuracy: 0.9142\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 13s 592ms/step - loss: 0.3955 - accuracy: 0.9152 - val_loss: nan - val_accuracy: 0.9172\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 13s 593ms/step - loss: 0.3771 - accuracy: 0.9179 - val_loss: nan - val_accuracy: 0.9187\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 13s 600ms/step - loss: 0.3624 - accuracy: 0.9192 - val_loss: nan - val_accuracy: 0.9201\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 13s 604ms/step - loss: 0.3497 - accuracy: 0.9206 - val_loss: nan - val_accuracy: 0.9216\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 13s 602ms/step - loss: 0.3379 - accuracy: 0.9224 - val_loss: nan - val_accuracy: 0.9236\n",
            "chine</w> est</w> généralement</w> en</w> en</w> mais</w> en</w> il</w> est</w> est</w> est</w> en</w> mais</w> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn9onhbnh3nU"
      },
      "source": [
        "### Accuracy \n",
        "\n",
        "Here we compare the accuracy of both models and we can clearly see that the accuracy of model using byte pair encoding is higher than word tokenized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0FqlFJCh2Pc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "661171b1-6d82-4713-896f-030dbd4407ce"
      },
      "source": [
        "acc = hist.history['val_accuracy']\n",
        "acc_bpe = hist_bpe.history['val_accuracy']\n",
        "epochs = range(1,10)\n",
        "plt.plot(acc_bpe, 'b', label='BPE model')\n",
        "plt.plot(acc, 'g', label='Word Tokenization model')\n",
        "plt.grid()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ydNrSd8YVnz"
      },
      "source": [
        "### RNN with Embedding\n",
        "\n",
        "Small changes to the model can increase the accuracy\n",
        "\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bF-BmOA4rUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d4586b-72e3-4e46-b902-cdd2114c62da"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#test_embed_model(embed_model)\n",
        "\n",
        "# TODO: Reshape the input\n",
        "tmp_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "# TODO: Train the neural network\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences_bpe.shape[1],\n",
        "    english_vocab_size_bpe+1,\n",
        "    french_vocab_size_bpe+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences_bpe, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# TODO: Print prediction(s)\n",
        "print(logits_to_text_bpe(embed_rnn_model.predict(tmp_x[:1])[0], vocab_tokenization_fr))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 115, 256)          51200     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 115, 256)          394752    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 115, 1024)         263168    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 115, 1024)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 115, 345)          353625    \n",
            "=================================================================\n",
            "Total params: 1,062,745\n",
            "Trainable params: 1,062,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "22/22 [==============================] - 10s 366ms/step - loss: 2.0656 - accuracy: 0.7511 - val_loss: 0.4711 - val_accuracy: 0.9043\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 7s 340ms/step - loss: 0.4448 - accuracy: 0.9065 - val_loss: 0.3697 - val_accuracy: 0.9168\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 7s 340ms/step - loss: 0.3487 - accuracy: 0.9200 - val_loss: 0.2742 - val_accuracy: 0.9337\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 8s 342ms/step - loss: 0.2656 - accuracy: 0.9335 - val_loss: 0.2170 - val_accuracy: 0.9427\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 8s 344ms/step - loss: 0.2112 - accuracy: 0.9439 - val_loss: 0.1704 - val_accuracy: 0.9542\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 8s 344ms/step - loss: 0.1726 - accuracy: 0.9528 - val_loss: 0.1404 - val_accuracy: 0.9607\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 7s 340ms/step - loss: 0.1427 - accuracy: 0.9593 - val_loss: 0.1198 - val_accuracy: 0.9651\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 7s 338ms/step - loss: 0.1239 - accuracy: 0.9637 - val_loss: 0.1072 - val_accuracy: 0.9678\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 7s 337ms/step - loss: 0.1093 - accuracy: 0.9672 - val_loss: 0.0958 - val_accuracy: 0.9708\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 7s 336ms/step - loss: 0.1010 - accuracy: 0.9690 - val_loss: 0.0894 - val_accuracy: 0.9721\n",
            "chine</w> est</w> généralement</w> occupé</w> en</w> septembre</w> mais</w> il</w> est</w> parfois</w> froid</w> en</w> printemps</w> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7CO6SRptF74"
      },
      "source": [
        "### Test the model on the test data set\n",
        "\n",
        "See how the model trained on the training set data performs on the unseen test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J82ut0wC4-eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60a4474-3fec-4ea6-c0e0-ddb07ce2a613"
      },
      "source": [
        "preproc_english_sentences_bpe, preproc_french_sentences_bpe, english_tokenizer_bpe, french_tokenizer_bpe=preprocess_BPE(english_sentences_test, french_sentences_test)\n",
        "\n",
        "test_x = preproc_english_sentences_bpe.reshape((-1, preproc_french_sentences_bpe.shape[-2]))\n",
        "\n",
        "embedding_model_score = embed_rnn_model.evaluate(test_x, preproc_french_sentences_bpe, verbose=0)\n",
        "\n",
        "print(\"Embedding model accuracy on unseen test data: {0:.2f}%\".format(embedding_model_score[1]*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding model accuracy on unseen test data: 97.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC73epV3ZIYN"
      },
      "source": [
        "##Enhancements\n",
        "\n",
        "This project focuses on using Byte-Pair encoding, we can use this tokenization method with different network architectures for machine translation and evaluate the models according to best practices by splitting the data into separate test & training sets. Every requirement is different, based on it we can adjust the network architecture and hyperparameters before finalizing the model."
      ]
    }
  ]
}